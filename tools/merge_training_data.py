"""
åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®åˆ°ç»Ÿä¸€æ–‡ä»¶å¤¹
"""
import shutil
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import json


def merge_training_data(project_root, output_name='candy_merged'):
    """
    åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®åˆ°ç»Ÿä¸€æ–‡ä»¶å¤¹
    """
    project_root = Path(project_root)
    
    # æºç›®å½•
    images_root = project_root / 'datasets' / 'extracted_frames'
    labels_root = project_root / 'datasets' / 'annotated' / 'labels'
    
    # ç›®æ ‡ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = project_root / 'datasets' / f'{output_name}_{timestamp}'
    
    merged_images = output_dir / 'images'
    merged_labels = output_dir / 'labels'
    
    merged_images.mkdir(parents=True, exist_ok=True)
    merged_labels.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ”„ åˆå¹¶è®­ç»ƒæ•°æ®")
    print(f"æºå›¾ç‰‡ç›®å½•: {images_root}")
    print(f"æºæ ‡ç­¾ç›®å½•: {labels_root}")
    print(f"ç›®æ ‡ç›®å½•: {output_dir}")
    print("-" * 60)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_images': 0,
        'total_labels': 0,
        'folders': defaultdict(lambda: {'images': 0, 'labels': 0}),
        'skipped': []
    }
    
    file_counter = 0
    
    # æ‰«ææ‰€æœ‰å­æ–‡ä»¶å¤¹
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
    
    for img_file in images_root.rglob('*'):
        if img_file.suffix.lower() in image_extensions:
            # è·å–ç›¸å¯¹è·¯å¾„
            rel_path = img_file.relative_to(images_root)
            folder_name = rel_path.parts[0] if len(rel_path.parts) > 1 else 'root'
            
            # å¯¹åº”çš„æ ‡ç­¾æ–‡ä»¶
            label_file = labels_root / rel_path.parent / f"{img_file.stem}.txt"
            
            # åªå¤åˆ¶æœ‰æ ‡æ³¨çš„å›¾ç‰‡
            if label_file.exists() and label_file.stat().st_size > 0:
                # ç”Ÿæˆæ–°æ–‡ä»¶åï¼ˆé¿å…å†²çªï¼‰
                new_name = f"{folder_name}_{img_file.stem}{img_file.suffix}"
                
                # å¦‚æœæ–‡ä»¶åè¿˜æ˜¯å†²çªï¼Œæ·»åŠ è®¡æ•°å™¨
                target_img = merged_images / new_name
                target_label = merged_labels / f"{Path(new_name).stem}.txt"
                
                if target_img.exists():
                    file_counter += 1
                    new_name = f"{folder_name}_{img_file.stem}_{file_counter:04d}{img_file.suffix}"
                    target_img = merged_images / new_name
                    target_label = merged_labels / f"{Path(new_name).stem}.txt"
                
                try:
                    # å¤åˆ¶å›¾ç‰‡å’Œæ ‡ç­¾
                    shutil.copy2(img_file, target_img)
                    shutil.copy2(label_file, target_label)
                    
                    stats['total_images'] += 1
                    stats['total_labels'] += 1
                    stats['folders'][folder_name]['images'] += 1
                    stats['folders'][folder_name]['labels'] += 1
                    
                except Exception as e:
                    print(f"âš ï¸  å¤åˆ¶å¤±è´¥: {rel_path} - {e}")
                    stats['skipped'].append(str(rel_path))
    
    # ç”Ÿæˆç±»åˆ«æ–‡ä»¶
    classes_file = output_dir / 'classes.txt'
    with open(classes_file, 'w', encoding='utf-8') as f:
        f.write("normal\n")
        f.write("abnormal\n")
    
    # ç”Ÿæˆæ•°æ®é›†é…ç½®
    yaml_file = output_dir / 'dataset.yaml'
    with open(yaml_file, 'w', encoding='utf-8') as f:
        f.write(f"# Merged Training Data - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"path: {output_dir.absolute()}\n")
        f.write("train: images  # æ‰€æœ‰æ•°æ®åœ¨åŒä¸€æ–‡ä»¶å¤¹\n")
        f.write("val: images    # è®­ç»ƒæ—¶å¯ä»¥è®¾ç½®éªŒè¯é›†æ¯”ä¾‹\n\n")
        f.write("names:\n")
        f.write("  0: normal\n")
        f.write("  1: abnormal\n\n")
        f.write("nc: 2\n")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    report_file = output_dir / 'merge_report.txt'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("è®­ç»ƒæ•°æ®åˆå¹¶æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"åˆå¹¶æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"è¾“å‡ºç›®å½•: {output_dir}\n\n")
        f.write(f"æ€»å›¾ç‰‡æ•°: {stats['total_images']}\n")
        f.write(f"æ€»æ ‡ç­¾æ•°: {stats['total_labels']}\n\n")
        f.write("å„æ–‡ä»¶å¤¹ç»Ÿè®¡:\n")
        for folder, counts in sorted(stats['folders'].items()):
            f.write(f"  {folder}:\n")
            f.write(f"    å›¾ç‰‡: {counts['images']}\n")
            f.write(f"    æ ‡ç­¾: {counts['labels']}\n")
        
        if stats['skipped']:
            f.write(f"\nè·³è¿‡çš„æ–‡ä»¶ ({len(stats['skipped'])}):\n")
            for skipped in stats['skipped']:
                f.write(f"  - {skipped}\n")
    
    # ä¿å­˜JSONç»Ÿè®¡
    json_file = output_dir / 'merge_stats.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump({
            'total_images': stats['total_images'],
            'total_labels': stats['total_labels'],
            'folders': dict(stats['folders']),
            'skipped_count': len(stats['skipped']),
            'merged_at': datetime.now().isoformat()
        }, f, indent=2, ensure_ascii=False)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "=" * 60)
    print("âœ… åˆå¹¶å®Œæˆï¼")
    print("=" * 60)
    print(f"æ€»å›¾ç‰‡æ•°: {stats['total_images']}")
    print(f"æ€»æ ‡ç­¾æ•°: {stats['total_labels']}")
    print(f"\nå„æ–‡ä»¶å¤¹ç»Ÿè®¡:")
    for folder, counts in sorted(stats['folders'].items()):
        print(f"  {folder}: {counts['images']} å¼ ")
    
    if stats['skipped']:
        print(f"\nâš ï¸  è·³è¿‡ {len(stats['skipped'])} ä¸ªæ–‡ä»¶")
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {report_file}")
    print(f"ğŸ“„ æ•°æ®é›†é…ç½®: {yaml_file}")
    print(f"ğŸ·ï¸  ç±»åˆ«æ–‡ä»¶: {classes_file}")
    
    return output_dir


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆå¹¶æ‰€æœ‰è®­ç»ƒæ•°æ®")
    parser.add_argument("--root", "-r", default=".", help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--name", "-n", default="candy_merged", help="è¾“å‡ºæ–‡ä»¶å¤¹åç§°å‰ç¼€")
    
    args = parser.parse_args()
    
    output_dir = merge_training_data(args.root, args.name)
    
    # æ‰“å¼€è¾“å‡ºç›®å½•
    import subprocess
    subprocess.run(['explorer', str(output_dir)], shell=True)
