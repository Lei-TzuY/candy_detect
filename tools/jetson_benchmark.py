"""
Jetson TX2 æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹è¯•ä¸åŒæ¨¡å‹æ ¼å¼çš„æ¨ç†é€Ÿåº¦å’Œèµ„æºå ç”¨
"""
from ultralytics import YOLO
import cv2
import time
import numpy as np
from pathlib import Path
import psutil
import os

def get_gpu_memory():
    """è·å– GPU å†…å­˜ä½¿ç”¨ï¼ˆJetson ä¸“ç”¨ï¼‰"""
    try:
        with open('/proc/driver/nvidia/gpuinfo', 'r') as f:
            info = f.read()
            # è¿™æ˜¯ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…éœ€è¦è§£ææ›´å¤šä¿¡æ¯
            return "GPU Info Available"
    except:
        return "N/A"

def benchmark_model(model_path, num_frames=100):
    """
    å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        num_frames: æµ‹è¯•å¸§æ•°
    
    Returns:
        dict: æ€§èƒ½æŒ‡æ ‡
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_path}")
    print(f"{'='*60}")
    
    if not Path(model_path).exists():
        print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        return None
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½æ¨¡å‹...")
    load_start = time.time()
    model = YOLO(model_path)
    load_time = time.time() - load_start
    print(f"âœ… æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}s")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
    
    # é¢„çƒ­
    print("é¢„çƒ­ä¸­...")
    for _ in range(10):
        _ = model.predict(test_image, verbose=False, device=0)
    
    # åŸºå‡†æµ‹è¯•
    print(f"è¿è¡Œ {num_frames} å¸§åŸºå‡†æµ‹è¯•...")
    inference_times = []
    cpu_usage = []
    memory_usage = []
    
    process = psutil.Process(os.getpid())
    
    for i in range(num_frames):
        # CPU å’Œå†…å­˜ç›‘æ§
        cpu_usage.append(psutil.cpu_percent(interval=None))
        memory_usage.append(process.memory_info().rss / 1024 / 1024)  # MB
        
        # æ¨ç†
        start_time = time.time()
        results = model.predict(test_image, verbose=False, device=0, conf=0.3)
        inference_time = (time.time() - start_time) * 1000  # ms
        inference_times.append(inference_time)
        
        if (i + 1) % 20 == 0:
            print(f"  è¿›åº¦: {i+1}/{num_frames}")
    
    # è®¡ç®—ç»Ÿè®¡
    results = {
        'model_path': model_path,
        'model_format': Path(model_path).suffix,
        'load_time': load_time,
        'avg_inference': np.mean(inference_times),
        'std_inference': np.std(inference_times),
        'min_inference': np.min(inference_times),
        'max_inference': np.max(inference_times),
        'avg_fps': 1000 / np.mean(inference_times),
        'avg_cpu': np.mean(cpu_usage),
        'avg_memory': np.mean(memory_usage),
        'p50_inference': np.percentile(inference_times, 50),
        'p95_inference': np.percentile(inference_times, 95),
        'p99_inference': np.percentile(inference_times, 99),
    }
    
    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
    print(f"  æ¨¡å‹æ ¼å¼: {results['model_format']}")
    print(f"  åŠ è½½æ—¶é—´: {results['load_time']:.2f}s")
    print(f"  å¹³å‡æ¨ç†: {results['avg_inference']:.2f}ms Â± {results['std_inference']:.2f}ms")
    print(f"  å»¶è¿ŸèŒƒå›´: {results['min_inference']:.2f}ms ~ {results['max_inference']:.2f}ms")
    print(f"  P50 å»¶è¿Ÿ: {results['p50_inference']:.2f}ms")
    print(f"  P95 å»¶è¿Ÿ: {results['p95_inference']:.2f}ms")
    print(f"  P99 å»¶è¿Ÿ: {results['p99_inference']:.2f}ms")
    print(f"  å¹³å‡ FPS: {results['avg_fps']:.2f}")
    print(f"  CPU ä½¿ç”¨: {results['avg_cpu']:.1f}%")
    print(f"  å†…å­˜ä½¿ç”¨: {results['avg_memory']:.1f}MB")
    
    return results

def main():
    """æµ‹è¯•æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹æ ¼å¼"""
    # å¯èƒ½çš„æ¨¡å‹è·¯å¾„
    model_candidates = [
        'best.pt',
        'best.onnx',
        'best.torchscript',
        'yolov8n_candy_fp16.engine',
        'best_fp16.engine'
    ]
    
    all_results = []
    
    print("ğŸš€ Jetson TX2 YOLOv8n æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 60)
    
    # ç³»ç»Ÿä¿¡æ¯
    print("\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPU: {psutil.cpu_count()} æ ¸")
    print(f"  å†…å­˜: {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")
    print(f"  å¯ç”¨å†…å­˜: {psutil.virtual_memory().available / 1024 / 1024 / 1024:.1f}GB")
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for model_path in model_candidates:
        if Path(model_path).exists():
            result = benchmark_model(model_path, num_frames=100)
            if result:
                all_results.append(result)
        else:
            print(f"\nâ­ï¸  è·³è¿‡: {model_path} (ä¸å­˜åœ¨)")
    
    # å¯¹æ¯”ç»“æœ
    if all_results:
        print("\n" + "=" * 60)
        print("ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print("=" * 60)
        
        # æŒ‰ FPS æ’åº
        all_results.sort(key=lambda x: x['avg_fps'], reverse=True)
        
        print(f"\n{'æ ¼å¼':<15} {'å¹³å‡æ¨ç†':<12} {'FPS':<8} {'å†…å­˜':<10}")
        print("-" * 60)
        for r in all_results:
            format_name = r['model_format']
            print(f"{format_name:<15} {r['avg_inference']:>8.2f}ms   {r['avg_fps']:>6.2f}   {r['avg_memory']:>7.1f}MB")
        
        print("\nğŸ’¡ æ¨è:")
        best = all_results[0]
        print(f"  æœ€å¿«æ¨¡å‹: {best['model_format']} ({best['avg_fps']:.2f} FPS)")
        print(f"  é¢„æœŸæ€§èƒ½: {1000/best['avg_inference']:.1f} FPS å®æ—¶æ£€æµ‹")
        
        # ä¿å­˜ç»“æœ
        import json
        with open('jetson_benchmark_results.json', 'w') as f:
            json.dump(all_results, f, indent=2)
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: jetson_benchmark_results.json")
    else:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•å¯æµ‹è¯•çš„æ¨¡å‹")
        print("\nè¯·å…ˆè¿è¡Œ export_for_jetson.py å¯¼å‡ºæ¨¡å‹")

if __name__ == '__main__':
    main()
