"""
å¯è§†åŒ–éªŒè¯é›†çš„æ ‡æ³¨ç»“æœå’Œæ¨¡å‹é¢„æµ‹å¯¹æ¯”
"""
import cv2
import numpy as np
from pathlib import Path
import yaml
from ultralytics import YOLO
import shutil
from datetime import datetime


def load_yolo_annotations(label_path, img_width, img_height):
    """åŠ è½½YOLOæ ¼å¼æ ‡æ³¨"""
    annotations = []
    if not Path(label_path).exists():
        return annotations
    
    with open(label_path, 'r') as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                class_id = int(parts[0])
                x_center = float(parts[1]) * img_width
                y_center = float(parts[2]) * img_height
                width = float(parts[3]) * img_width
                height = float(parts[4]) * img_height
                
                x1 = int(x_center - width / 2)
                y1 = int(y_center - height / 2)
                x2 = int(x_center + width / 2)
                y2 = int(y_center + height / 2)
                
                annotations.append({
                    'class_id': class_id,
                    'bbox': [x1, y1, x2, y2]
                })
    
    return annotations


def visualize_validation_set(dataset_yaml, model_path=None, output_dir=None, max_images=50):
    """å¯è§†åŒ–éªŒè¯é›†çš„æ ‡æ³¨å’Œé¢„æµ‹ç»“æœ"""
    
    # è¯»å–æ•°æ®é›†é…ç½®
    with open(dataset_yaml, 'r', encoding='utf-8') as f:
        dataset_config = yaml.safe_load(f)
    
    # è·å–è·¯å¾„
    dataset_path = Path(dataset_yaml).parent
    val_images_path = dataset_path / dataset_config['val']
    
    # ç±»åˆ«åç§°
    class_names = dataset_config['names']
    colors = {
        0: (0, 255, 0),    # normal - ç»¿è‰²
        1: (0, 0, 255),    # abnormal - çº¢è‰²
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(f"visualizations/validation_{timestamp}")
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœæä¾›ï¼‰
    model = None
    if model_path and Path(model_path).exists():
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        model = YOLO(model_path)
    
    # è·å–æ‰€æœ‰éªŒè¯å›¾ç‰‡
    image_files = list(val_images_path.glob('*.jpg')) + list(val_images_path.glob('*.png'))
    print(f"ğŸ“Š æ‰¾åˆ° {len(image_files)} å¼ éªŒè¯å›¾ç‰‡")
    
    # é™åˆ¶æ•°é‡
    if len(image_files) > max_images:
        print(f"âš ï¸ é™åˆ¶æ˜¾ç¤ºå‰ {max_images} å¼ å›¾ç‰‡")
        image_files = image_files[:max_images]
    
    # å¤„ç†æ¯å¼ å›¾ç‰‡
    processed = 0
    for img_path in image_files:
        try:
            # è¯»å–å›¾ç‰‡
            img = cv2.imread(str(img_path))
            if img is None:
                continue
            
            height, width = img.shape[:2]
            
            # åˆ›å»ºä¸¤ä¸ªå‰¯æœ¬ï¼šä¸€ä¸ªæ˜¾ç¤ºground truthï¼Œä¸€ä¸ªæ˜¾ç¤ºé¢„æµ‹
            img_gt = img.copy()
            img_pred = img.copy() if model else None
            
            # 1. ç»˜åˆ¶Ground Truthæ ‡æ³¨
            label_path = val_images_path.parent / 'labels' / f"{img_path.stem}.txt"
            gt_annotations = load_yolo_annotations(label_path, width, height)
            
            for ann in gt_annotations:
                class_id = ann['class_id']
                x1, y1, x2, y2 = ann['bbox']
                color = colors.get(class_id, (255, 255, 255))
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(img_gt, (x1, y1), (x2, y2), color, 2)
                
                # ç»˜åˆ¶æ ‡ç­¾
                label = f"GT: {class_names[class_id]}"
                (text_width, text_height), baseline = cv2.getTextSize(
                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
                )
                cv2.rectangle(
                    img_gt,
                    (x1, y1 - text_height - baseline - 5),
                    (x1 + text_width, y1),
                    color,
                    -1
                )
                cv2.putText(
                    img_gt, label, (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1
                )
            
            # 2. è¿è¡Œæ¨¡å‹é¢„æµ‹ï¼ˆå¦‚æœæœ‰æ¨¡å‹ï¼‰
            if model:
                results = model.predict(img_path, conf=0.25, verbose=False)
                
                for result in results:
                    boxes = result.boxes
                    for box in boxes:
                        # è·å–åæ ‡å’Œç±»åˆ«
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
                        conf = float(box.conf[0])
                        class_id = int(box.cls[0])
                        color = colors.get(class_id, (255, 255, 255))
                        
                        # ç»˜åˆ¶è¾¹ç•Œæ¡†
                        cv2.rectangle(img_pred, (x1, y1), (x2, y2), color, 2)
                        
                        # ç»˜åˆ¶æ ‡ç­¾
                        label = f"Pred: {class_names[class_id]} {conf:.2f}"
                        (text_width, text_height), baseline = cv2.getTextSize(
                            label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
                        )
                        cv2.rectangle(
                            img_pred,
                            (x1, y1 - text_height - baseline - 5),
                            (x1 + text_width, y1),
                            color,
                            -1
                        )
                        cv2.putText(
                            img_pred, label, (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1
                        )
            
            # 3. åˆå¹¶æ˜¾ç¤ºï¼ˆå¦‚æœæœ‰é¢„æµ‹ç»“æœï¼‰
            if model and img_pred is not None:
                # åˆ›å»ºå¹¶æ’å¯¹æ¯”å›¾
                # æ·»åŠ æ ‡é¢˜
                title_height = 40
                combined_height = max(img_gt.shape[0], img_pred.shape[0]) + title_height
                combined_width = img_gt.shape[1] + img_pred.shape[1]
                combined = np.ones((combined_height, combined_width, 3), dtype=np.uint8) * 255
                
                # æ·»åŠ æ ‡é¢˜æ–‡å­—
                cv2.putText(combined, "Ground Truth", (img_gt.shape[1]//2 - 80, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
                cv2.putText(combined, "Prediction", (img_gt.shape[1] + img_pred.shape[1]//2 - 80, 30),
                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2)
                
                # æ”¾ç½®å›¾ç‰‡
                combined[title_height:title_height+img_gt.shape[0], :img_gt.shape[1]] = img_gt
                combined[title_height:title_height+img_pred.shape[0], img_gt.shape[1]:] = img_pred
                
                # ä¿å­˜å¯¹æ¯”å›¾
                output_path = output_dir / f"{img_path.stem}_comparison.jpg"
                cv2.imwrite(str(output_path), combined)
            else:
                # åªä¿å­˜ground truth
                output_path = output_dir / f"{img_path.stem}_gt.jpg"
                cv2.imwrite(str(output_path), img_gt)
            
            processed += 1
            if processed % 10 == 0:
                print(f"âœ… å·²å¤„ç† {processed}/{len(image_files)} å¼ å›¾ç‰‡")
                
        except Exception as e:
            print(f"âŒ å¤„ç† {img_path.name} å¤±è´¥: {e}")
            continue
    
    print(f"\nâœ… å¯è§†åŒ–å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"ğŸ“Š å…±å¤„ç† {processed} å¼ å›¾ç‰‡")
    
    # ç”ŸæˆHTMLç´¢å¼•é¡µé¢
    generate_html_index(output_dir, class_names)
    
    return output_dir


def generate_html_index(output_dir, class_names):
    """ç”ŸæˆHTMLç´¢å¼•é¡µé¢"""
    image_files = sorted(output_dir.glob('*.jpg'))
    
    html = """<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>éªŒè¯é›†å¯è§†åŒ–ç»“æœ</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            background: #f5f7fa;
            padding: 20px;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
        }
        h1 {
            color: #2d3748;
            margin-bottom: 20px;
            text-align: center;
        }
        .legend {
            background: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .legend-item {
            display: inline-block;
            margin-right: 20px;
            padding: 5px 15px;
            border-radius: 5px;
            font-weight: bold;
        }
        .normal {
            background: #48bb78;
            color: white;
        }
        .abnormal {
            background: #f56565;
            color: white;
        }
        .gallery {
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(600px, 1fr));
            gap: 20px;
        }
        .image-card {
            background: white;
            border-radius: 10px;
            padding: 15px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            transition: transform 0.3s;
        }
        .image-card:hover {
            transform: translateY(-5px);
        }
        .image-card img {
            width: 100%;
            border-radius: 5px;
        }
        .image-card .filename {
            margin-top: 10px;
            color: #718096;
            font-size: 0.9em;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¬ éªŒè¯é›†æ ‡æ³¨å¯è§†åŒ–ç»“æœ</h1>
        
        <div class="legend">
            <h3>å›¾ä¾‹è¯´æ˜ï¼š</h3>
            <span class="legend-item normal">Normal (æ­£å¸¸)</span>
            <span class="legend-item abnormal">Abnormal (å¼‚å¸¸)</span>
        </div>
        
        <div class="gallery">
"""
    
    for img_file in image_files:
        html += f"""
            <div class="image-card">
                <img src="{img_file.name}" alt="{img_file.stem}">
                <div class="filename">{img_file.name}</div>
            </div>
"""
    
    html += """
        </div>
    </div>
</body>
</html>
"""
    
    index_path = output_dir / 'index.html'
    with open(index_path, 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"ğŸ“„ HTMLç´¢å¼•é¡µé¢: {index_path}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–éªŒè¯é›†æ ‡æ³¨ç»“æœ')
    parser.add_argument('--dataset', type=str, 
                       default='datasets/candy_merged_20260116_154158/dataset.yaml',
                       help='æ•°æ®é›†YAMLæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str,
                       default='runs/detect/runs/train/candy_detector3/weights/best.pt',
                       help='è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--max-images', type=int, default=50,
                       help='æœ€å¤šå¯è§†åŒ–å¤šå°‘å¼ å›¾ç‰‡')
    
    args = parser.parse_args()
    
    output_dir = visualize_validation_set(
        args.dataset,
        args.model if Path(args.model).exists() else None,
        args.output,
        args.max_images
    )
    
    # æ‰“å¼€ç»“æœ
    import subprocess
    subprocess.run(['explorer', str(output_dir)], shell=True)
