"""
ä¿®æ­£æ ‡æ³¨é—®é¢˜
1. åˆ é™¤è¾¹ç•Œæ¡†è¿‡å¤§çš„ç…§ç‰‡
2. åˆ é™¤é•¿å®½æ¯”å¼‚å¸¸çš„ç…§ç‰‡
3. ä¿®æ­£è¶…å‡ºè¾¹ç•Œçš„æ ‡æ³¨ï¼ˆclipåˆ°è¾¹ç•Œå†…ï¼‰
"""
import os
import json
import shutil
from pathlib import Path

def fix_annotations(dataset_dir, report_file='annotation_quality_report.json', backup=True):
    """ä¿®æ­£æ ‡æ³¨é—®é¢˜"""
    
    # è¯»å–æŠ¥å‘Š
    with open(report_file, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    issues = report['issues']
    
    dataset_path = Path(dataset_dir)
    images_dir = dataset_path / 'images'
    labels_dir = dataset_path / 'labels'
    
    print("=" * 70)
    print("ğŸ”§ ä¿®æ­£æ ‡æ³¨é—®é¢˜")
    print("=" * 70)
    print(f"\nğŸ“ æ•°æ®é›†: {dataset_dir}")
    
    # å¤‡ä»½
    if backup:
        backup_dir = Path(f"{dataset_dir}_backup_{report['stats']['total_images']}")
        if not backup_dir.exists():
            print(f"\nğŸ’¾ åˆ›å»ºå¤‡ä»½: {backup_dir}")
            shutil.copytree(dataset_dir, backup_dir)
            print("   âœ… å¤‡ä»½å®Œæˆ")
    
    deleted_count = 0
    fixed_count = 0
    
    # 1. åˆ é™¤è¾¹ç•Œæ¡†è¿‡å¤§çš„ç…§ç‰‡
    if issues['too_large']:
        print(f"\nğŸ—‘ï¸  åˆ é™¤è¾¹ç•Œæ¡†è¿‡å¤§çš„ç…§ç‰‡ ({len(issues['too_large'])} å¼ )...")
        for item in issues['too_large']:
            img_name = item['image']
            img_path = images_dir / img_name
            label_path = labels_dir / (Path(img_name).stem + '.txt')
            
            if img_path.exists():
                img_path.unlink()
                deleted_count += 1
                print(f"   âŒ {img_name} (é¢ç§¯ {item['area_percent']:.1f}%)")
            
            if label_path.exists():
                label_path.unlink()
        
        print(f"   âœ… å·²åˆ é™¤ {len(issues['too_large'])} å¼ ")
    
    # 2. åˆ é™¤é•¿å®½æ¯”å¼‚å¸¸çš„ç…§ç‰‡
    if issues['abnormal_aspect']:
        print(f"\nğŸ—‘ï¸  åˆ é™¤é•¿å®½æ¯”å¼‚å¸¸çš„ç…§ç‰‡ ({len(issues['abnormal_aspect'])} å¼ )...")
        
        # æŒ‰å›¾ç‰‡åˆ†ç»„
        img_set = set()
        for item in issues['abnormal_aspect']:
            img_set.add(item['image'])
        
        for img_name in img_set:
            img_path = images_dir / img_name
            label_path = labels_dir / (Path(img_name).stem + '.txt')
            
            if img_path.exists():
                img_path.unlink()
                deleted_count += 1
                print(f"   âŒ {img_name}")
            
            if label_path.exists():
                label_path.unlink()
        
        print(f"   âœ… å·²åˆ é™¤ {len(img_set)} å¼ ")
    
    # 3. ä¿®æ­£è¶…å‡ºè¾¹ç•Œçš„æ ‡æ³¨ï¼ˆclip åˆ° 0-1 èŒƒå›´ï¼‰
    if issues['out_of_bounds']:
        print(f"\nğŸ”§ ä¿®æ­£è¶…å‡ºè¾¹ç•Œçš„æ ‡æ³¨ ({len(issues['out_of_bounds'])} ä¸ª)...")
        
        # æŒ‰å›¾ç‰‡åˆ†ç»„
        img_to_fix = {}
        for item in issues['out_of_bounds']:
            img_name = item['image']
            if img_name not in img_to_fix:
                img_to_fix[img_name] = []
            img_to_fix[img_name].append(item)
        
        for img_name in img_to_fix.keys():
            label_path = labels_dir / (Path(img_name).stem + '.txt')
            
            if not label_path.exists():
                continue
            
            # è¯»å–æ‰€æœ‰æ ‡æ³¨
            fixed_lines = []
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 5:
                        class_id = int(parts[0])
                        x_center = float(parts[1])
                        y_center = float(parts[2])
                        width = float(parts[3])
                        height = float(parts[4])
                        
                        # Clip åˆ° 0-1 èŒƒå›´
                        x_min = max(0.0, x_center - width/2)
                        y_min = max(0.0, y_center - height/2)
                        x_max = min(1.0, x_center + width/2)
                        y_max = min(1.0, y_center + height/2)
                        
                        # é‡æ–°è®¡ç®—ä¸­å¿ƒå’Œå®½é«˜
                        new_x_center = (x_min + x_max) / 2
                        new_y_center = (y_min + y_max) / 2
                        new_width = x_max - x_min
                        new_height = y_max - y_min
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆï¼ˆå®½é«˜ > 0ï¼‰
                        if new_width > 0 and new_height > 0:
                            fixed_lines.append(f"{class_id} {new_x_center:.6f} {new_y_center:.6f} {new_width:.6f} {new_height:.6f}\n")
            
            # å†™å›æ–‡ä»¶
            if fixed_lines:
                with open(label_path, 'w') as f:
                    f.writelines(fixed_lines)
                fixed_count += 1
        
        print(f"   âœ… å·²ä¿®æ­£ {fixed_count} å¼ å›¾ç‰‡çš„æ ‡æ³¨")
    
    # ç»Ÿè®¡ç»“æœ
    remaining_images = len(list(images_dir.glob('*.jpg'))) + len(list(images_dir.glob('*.png')))
    remaining_labels = len(list(labels_dir.glob('*.txt')))
    
    print("\n" + "=" * 70)
    print("ğŸ“Š å¤„ç†ç»“æœ")
    print("=" * 70)
    print(f"\nåŸå§‹æ•°æ®:")
    print(f"   å›¾ç‰‡: {report['stats']['total_images']} å¼ ")
    print(f"   æ ‡æ³¨æ¡†: {report['stats']['total_boxes']} ä¸ª")
    
    print(f"\nåˆ é™¤:")
    print(f"   å›¾ç‰‡: {deleted_count} å¼ ")
    
    print(f"\nä¿®æ­£:")
    print(f"   æ ‡æ³¨æ–‡ä»¶: {fixed_count} ä¸ª")
    
    print(f"\nå‰©ä½™:")
    print(f"   å›¾ç‰‡: {remaining_images} å¼ ")
    print(f"   æ ‡æ³¨: {remaining_labels} ä¸ª")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("   1. ç°åœ¨å¯ä»¥ç”¨æ¨¡å‹é‡æ–°æ ‡è®°æ•´ä¸ªæ•°æ®é›†ï¼ˆè¡¥å……å¯èƒ½é—æ¼çš„æ ‡æ³¨ï¼‰")
    print("   2. æˆ–è€…ç›´æ¥ç”¨ä¿®æ­£åçš„æ•°æ®é›†é‡æ–°è®­ç»ƒ")
    
    return remaining_images, remaining_labels

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿®æ­£æ ‡æ³¨é—®é¢˜')
    parser.add_argument('--dataset', type=str, 
                        default='datasets/candy_merged_20260116_154158',
                        help='æ•°æ®é›†ç›®å½•è·¯å¾„')
    parser.add_argument('--report', type=str,
                        default='annotation_quality_report.json',
                        help='è´¨é‡æ£€æŸ¥æŠ¥å‘Šæ–‡ä»¶')
    parser.add_argument('--no-backup', action='store_true',
                        help='ä¸åˆ›å»ºå¤‡ä»½')
    
    args = parser.parse_args()
    
    fix_annotations(args.dataset, args.report, backup=not args.no_backup)
