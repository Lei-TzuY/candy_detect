"""
å°éŒ„è£½çš„å½±ç‰‡æª”æ¡ˆé€²è¡Œ YOLOv4 åµæ¸¬
å¯ä»¥æ‰¹æ¬¡è™•ç†å¤šå€‹å½±ç‰‡ï¼Œè¼¸å‡ºå¸¶æ¨™è¨»çš„å½±ç‰‡
"""
import cv2
import os
import sys
import configparser
from pathlib import Path
import tempfile
import shutil
import time


# å°ˆæ¡ˆæ ¹ç›®éŒ„
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))


def load_yolo_model(config_file='config.ini'):
    """è¼‰å…¥ YOLO æ¨¡å‹"""
    config = configparser.ConfigParser()
    config.read(config_file, encoding='utf-8')
    
    weights_path = os.path.normpath(os.path.join(PROJECT_ROOT, config.get('Paths', 'weights')))
    cfg_path = os.path.normpath(os.path.join(PROJECT_ROOT, config.get('Paths', 'cfg')))
    classes_path = os.path.normpath(os.path.join(PROJECT_ROOT, config.get('Paths', 'classes')))
    
    if not all(os.path.exists(p) for p in [weights_path, cfg_path, classes_path]):
        raise FileNotFoundError("æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆï¼Œè«‹æª¢æŸ¥ config.ini")
    
    with open(classes_path, 'r', encoding='utf-8') as f:
        class_names = [cname.strip() for cname in f.readlines()]
    
    # è™•ç†ä¸­æ–‡è·¯å¾‘å•é¡Œ
    temp_dir = os.path.join(tempfile.gettempdir(), 'candy_yolo_models')
    os.makedirs(temp_dir, exist_ok=True)
    temp_cfg = os.path.join(temp_dir, 'model.cfg')
    temp_weights = os.path.join(temp_dir, 'model.weights')
    
    if not os.path.exists(temp_cfg) or os.path.getsize(temp_cfg) != os.path.getsize(cfg_path):
        shutil.copy2(cfg_path, temp_cfg)
    
    if not os.path.exists(temp_weights) or os.path.getsize(temp_weights) != os.path.getsize(weights_path):
        shutil.copy2(weights_path, temp_weights)
    
    net = cv2.dnn.readNet(temp_weights, temp_cfg)
    model = cv2.dnn_DetectionModel(net)
    
    input_size = config.getint('Detection', 'input_size', fallback=416)
    confidence = config.getfloat('Detection', 'confidence_threshold', fallback=0.4)
    nms = config.getfloat('Detection', 'nms_threshold', fallback=0.4)
    
    model.setInputParams(size=(input_size, input_size), scale=1/255, swapRB=False)
    
    print(f"âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    print(f"  é¡åˆ¥: {class_names}")
    print(f"  è¼¸å…¥å¤§å°: {input_size}x{input_size}")
    print(f"  ä¿¡å¿ƒé–¾å€¼: {confidence}")
    print(f"  NMS é–¾å€¼: {nms}")
    
    return model, class_names, confidence, nms


def detect_video(video_path, output_path, model, class_names, 
                 confidence_threshold=0.4, nms_threshold=0.4,
                 show_preview=False, skip_frames=0):
    """
    å°å½±ç‰‡é€²è¡Œåµæ¸¬ä¸¦è¼¸å‡ºçµæœ
    
    Args:
        video_path: è¼¸å…¥å½±ç‰‡è·¯å¾‘
        output_path: è¼¸å‡ºå½±ç‰‡è·¯å¾‘
        model: YOLO æ¨¡å‹
        class_names: é¡åˆ¥åç¨±
        confidence_threshold: ä¿¡å¿ƒé–¾å€¼
        nms_threshold: NMS é–¾å€¼
        show_preview: æ˜¯å¦é¡¯ç¤ºå³æ™‚é è¦½
        skip_frames: è·³éå¹€æ•¸ï¼ˆåŠ å¿«è™•ç†ï¼Œ0=ä¸è·³éï¼‰
    """
    video_path = Path(video_path)
    output_path = Path(output_path)
    
    if not video_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ°å½±ç‰‡: {video_path}")
        return False
    
    # é–‹å•Ÿå½±ç‰‡
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        print(f"âŒ ç„¡æ³•é–‹å•Ÿå½±ç‰‡: {video_path}")
        return False
    
    # å–å¾—å½±ç‰‡è³‡è¨Š
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0
    
    print(f"\nğŸ“¹ å½±ç‰‡è³‡è¨Š:")
    print(f"   æª”æ¡ˆ: {video_path.name}")
    print(f"   è§£æåº¦: {width}x{height}")
    print(f"   FPS: {fps:.1f}")
    print(f"   ç¸½å¹€æ•¸: {total_frames}")
    print(f"   æ™‚é•·: {duration:.1f} ç§’")
    if skip_frames > 0:
        print(f"   è·³å¹€è¨­å®š: æ¯ {skip_frames+1} å¹€è™•ç† 1 å¹€")
    
    # å»ºç«‹è¼¸å‡ºå½±ç‰‡
    output_path.parent.mkdir(parents=True, exist_ok=True)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
    
    if not out.isOpened():
        print(f"âŒ ç„¡æ³•å»ºç«‹è¼¸å‡ºå½±ç‰‡: {output_path}")
        cap.release()
        return False
    
    print(f"\nğŸ¬ é–‹å§‹è™•ç†...")
    print(f"   è¼¸å‡º: {output_path}")
    print("=" * 60)
    
    frame_count = 0
    processed_count = 0
    detection_count = 0
    normal_count = 0
    abnormal_count = 0
    start_time = time.time()
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # è·³å¹€è™•ç†
            if skip_frames > 0 and frame_count % (skip_frames + 1) != 0:
                out.write(frame)  # å¯«å…¥åŸå§‹å¹€
                continue
            
            # è½‰ç°éšé€²è¡Œåµæ¸¬
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # åµæ¸¬
            classes, scores, boxes = model.detect(gray, confidence_threshold, nms_threshold)
            
            # ç¹ªè£½åµæ¸¬æ¡†
            output_frame = frame.copy()
            if len(classes) > 0:
                detection_count += len(classes)
                for class_id, score, box in zip(classes.flatten(), scores.flatten(), boxes):
                    x, y, w, h = box
                    class_name = class_names[class_id] if class_id < len(class_names) else f"Class {class_id}"
                    
                    # æ ¹æ“šé¡åˆ¥é¸æ“‡é¡è‰²
                    if class_id == 0:  # normal
                        color = (0, 255, 0)  # ç¶ è‰²
                        normal_count += 1
                    else:  # abnormal
                        color = (0, 0, 255)  # ç´…è‰²
                        abnormal_count += 1
                    
                    # ç¹ªè£½æ¡†å’Œæ¨™ç±¤
                    cv2.rectangle(output_frame, (x, y), (x+w, y+h), color, 2)
                    
                    # æ¨™ç±¤èƒŒæ™¯
                    label = f"{class_name}: {score:.2f}"
                    (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
                    cv2.rectangle(output_frame, (x, y-label_h-10), (x+label_w, y), color, -1)
                    cv2.putText(output_frame, label, (x, y-5), 
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
            # æ·»åŠ å¹€è³‡è¨Š
            info_text = f"Frame: {frame_count}/{total_frames} | Detections: {len(classes)}"
            cv2.putText(output_frame, info_text, (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            
            # å¯«å…¥è¼¸å‡ºå½±ç‰‡
            out.write(output_frame)
            processed_count += 1
            
            # é¡¯ç¤ºé è¦½
            if show_preview:
                cv2.imshow('Detection Preview (press Q to quit)', output_frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    print("\nâ¸ï¸ ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
                    break
            
            # é€²åº¦é¡¯ç¤º
            if frame_count % 100 == 0 or frame_count == total_frames:
                elapsed = time.time() - start_time
                fps_processing = frame_count / elapsed if elapsed > 0 else 0
                progress = (frame_count / total_frames) * 100
                eta = (total_frames - frame_count) / fps_processing if fps_processing > 0 else 0
                print(f"é€²åº¦: {progress:.1f}% ({frame_count}/{total_frames}) | "
                      f"è™•ç†é€Ÿåº¦: {fps_processing:.1f} FPS | "
                      f"é è¨ˆå‰©é¤˜: {eta:.0f}ç§’")
    
    except KeyboardInterrupt:
        print("\nâ¸ï¸ è™•ç†è¢«ä¸­æ–·")
    
    finally:
        cap.release()
        out.release()
        if show_preview:
            cv2.destroyAllWindows()
    
    elapsed_time = time.time() - start_time
    
    print("\n" + "=" * 60)
    print(f"âœ… è™•ç†å®Œæˆï¼")
    print(f"   è™•ç†å¹€æ•¸: {processed_count}/{frame_count}")
    print(f"   ç¸½åµæ¸¬æ•¸: {detection_count}")
    print(f"   æ­£å¸¸: {normal_count} | ç‘•ç–µ: {abnormal_count}")
    print(f"   è™•ç†æ™‚é–“: {elapsed_time:.1f} ç§’")
    print(f"   å¹³å‡é€Ÿåº¦: {frame_count/elapsed_time:.1f} FPS")
    print(f"\nğŸ“ è¼¸å‡ºæª”æ¡ˆ: {output_path}")
    print(f"   å¤§å°: {output_path.stat().st_size / (1024*1024):.1f} MB")
    
    return True


def batch_detect_videos(input_dir, output_dir, model, class_names,
                        confidence_threshold=0.4, nms_threshold=0.4,
                        skip_frames=0):
    """æ‰¹æ¬¡è™•ç†å¤šå€‹å½±ç‰‡"""
    input_dir = Path(input_dir)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ‰¾åˆ°æ‰€æœ‰å½±ç‰‡æª”æ¡ˆ
    video_files = list(input_dir.glob("*.mp4")) + list(input_dir.glob("*.avi"))
    
    if not video_files:
        print(f"âŒ åœ¨ {input_dir} ä¸­æ‰¾ä¸åˆ°å½±ç‰‡æª”æ¡ˆ")
        return
    
    print(f"\nğŸ“ æ‰¾åˆ° {len(video_files)} å€‹å½±ç‰‡æª”æ¡ˆ")
    print("=" * 60)
    
    success_count = 0
    for i, video_file in enumerate(video_files, 1):
        print(f"\n[{i}/{len(video_files)}] è™•ç†: {video_file.name}")
        output_file = output_dir / f"{video_file.stem}_detected{video_file.suffix}"
        
        success = detect_video(
            video_file, output_file, model, class_names,
            confidence_threshold, nms_threshold,
            show_preview=False, skip_frames=skip_frames
        )
        
        if success:
            success_count += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ æ‰¹æ¬¡è™•ç†å®Œæˆï¼")
    print(f"   æˆåŠŸ: {success_count}/{len(video_files)}")
    print(f"   è¼¸å‡ºç›®éŒ„: {output_dir}")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ä½¿ç”¨ YOLOv4 å°å½±ç‰‡é€²è¡Œåµæ¸¬')
    parser.add_argument('--video', type=str, help='å–®å€‹å½±ç‰‡æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--batch', type=str, help='æ‰¹æ¬¡è™•ç†ç›®éŒ„ï¼ˆå¦‚ recordingsï¼‰')
    parser.add_argument('--output', type=str, default='results/detected_videos',
                        help='è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: results/detected_videosï¼‰')
    parser.add_argument('--confidence', type=float, default=None,
                        help='ä¿¡å¿ƒé–¾å€¼ï¼ˆé è¨­ä½¿ç”¨ config.ini çš„è¨­å®šï¼‰')
    parser.add_argument('--nms', type=float, default=None,
                        help='NMS é–¾å€¼ï¼ˆé è¨­ä½¿ç”¨ config.ini çš„è¨­å®šï¼‰')
    parser.add_argument('--preview', action='store_true',
                        help='é¡¯ç¤ºå³æ™‚é è¦½ï¼ˆæŒ‰ Q åœæ­¢ï¼‰')
    parser.add_argument('--skip-frames', type=int, default=0,
                        help='è·³å¹€è™•ç†ï¼ˆ0=ä¸è·³éï¼Œ1=æ¯2å¹€è™•ç†1å¹€ï¼ŒåŠ å¿«è™•ç†ï¼‰')
    
    args = parser.parse_args()
    
    if not args.video and not args.batch:
        print("è«‹æŒ‡å®š --video æˆ– --batch åƒæ•¸")
        print("\nç¯„ä¾‹:")
        print("  å–®å€‹å½±ç‰‡: python detect_video.py --video recordings/recording_0.mp4")
        print("  æ‰¹æ¬¡è™•ç†: python detect_video.py --batch recordings")
        print("  å³æ™‚é è¦½: python detect_video.py --video recordings/recording_0.mp4 --preview")
        sys.exit(1)
    
    try:
        print("ğŸš€ è¼‰å…¥ YOLOv4 æ¨¡å‹...")
        model, class_names, default_conf, default_nms = load_yolo_model()
        
        confidence = args.confidence if args.confidence is not None else default_conf
        nms = args.nms if args.nms is not None else default_nms
        
        if args.video:
            # è™•ç†å–®å€‹å½±ç‰‡
            output_path = Path(args.output) / f"{Path(args.video).stem}_detected.mp4"
            detect_video(
                args.video, output_path, model, class_names,
                confidence, nms, args.preview, args.skip_frames
            )
        elif args.batch:
            # æ‰¹æ¬¡è™•ç†
            batch_detect_videos(
                args.batch, args.output, model, class_names,
                confidence, nms, args.skip_frames
            )
        
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
